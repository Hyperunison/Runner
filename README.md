This is stateless agent to run nextflow command or transfer debug data from cluster to hyperunison web platform

# Architecture
 - Agent is a communication point between Hyperunison platform and cluster inside any VPN network.
 - Agent connect only with Hyperunison platform and cluster. Any other network connections may be restricted by firewall. Communication with hyperunison is done using REST over https
 - Agent may run any nextflow code generated by Hyperunison platform. It uploads received nextflow code to separate folder and execute nextflow command
 - By default, Hyperunison pipelines upload pipeline results to AWS S3 to make them available to user requested pipeline. Credentials and path to S3 are provided by platform

# Technical details
## Code structure
- /Resources/ - folder to store creds and Dockerfiles
- /src/Adapters/ - folder to store adapter to different clusters. Now supporting k8s only
- /src/auto/ - SDK for communication with Hyperunison platform. **Autogenerated code**. Do not modify it manually! It is generated by swagger documentation of hyperunison API using update-api.sh script
- /src/Message/ - possibles messages from Hyperunison platform

## Types of tasks
 - Run nextflow command (NextflowRun). Parameters:
   - nextflow_code (string) - (will be uploaded to cluster to seperate folder)
   - run_id (int) - in hyperunison platform for sending run status
   - dataset_id (int) - in hyperunison platform for sending run status
   - command - (string) nextflow command to run
   - input_data (array) - a list of objects with metadata and links to fastq files. It is used as input to nextflow
   - aws_id, aws_key, aws_s3_path (string) - AWS credentials and S3 to upload results of nextflow
 - Get process logs (GetProcessLogs). Ask last log lines from process (pod in k8s or similar entity in another clusters) Parameters:
   - process_id (string)
   - reply_channel (string) - where to send response
   - lines_limit (int) - how many lines to process
 - Kill Job (KillJob). Kills currently working nextflow process if it is not required anymore. Parameters:
   - run_id (int) - what process to kill
   - channel (string) - where to send response

## Logs
 - All remote actions, like command execution or http requests are logged to logs
 - Logs are written to stdout by default

# Installation

## Using docker-compose
```commandline
$ docker-compose up -d
```

## Using singularity: 
//todo

## Cluster configuration

### AWS
 - generate credentials and put them to Resources/.aws folder. Folder should contain 2 files: `config` and `credentials`

### k8s
 - If you use k8s inside AWS, follow AWS instructions first
 - Copy k8s config to Resources/.kube folder. It should be one file: `config`
 - More about k8s config file you may read here: https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/

## Agent configuration
 - config file is `config.yaml`

# Development
## Update API generated code (SDK)
```
$ bash update-api.sh
```